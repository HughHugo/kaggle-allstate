---
title: "amex oct"
author: "Andy Chung"
date: "07 Sep 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Config
```{r results = 'hide', message = FALSE, warning = FALSE}
rm(list = ls())  # Remove all objects
gc()             # Garbage Collection

library(xgboost)   # XGboost for Machine Learning
library(dplyr)     # Grammar of Data Manipulation
library(lubridate) # Date time handle
library(caret)      # caret
library(data.table) # Fast aggregation of large data
library(readr)     # Read Tabular Data
library(Matrix)    # Sparse Matrix
library(tidyr)  

options(scipen=100)
options(dplyr.width = Inf)
#options(dplyr.print_max = Inf)

# **************************************
# functions
# **************************************
'%nin%' <- Negate('%in%')
'%in_v%' <- function(x, y) x[x %in% y] 
'%nin_v%' <- function(x, y) x[!x %in% y] 
'%in_d%' <- function(x, y) x[names(x) %in% y] 
'%nin_d%' <- function(x, y) x[!names(x) %in% y] 
'%+%' <- function(x, y) paste0(x, y)

Mean <- function(x) mean(x, na.rm=TRUE)
Median <- function(x) median(x, na.rm=TRUE)
Sd <- function(x) sd(x, na.rm=TRUE)
Sum <- function(x) sum(x, na.rm=TRUE)
Max <- function(x) max(x, na.rm=TRUE)
Min <- function(x) min(x, na.rm=TRUE)
Mean_value <- function(x) ifelse(is.nan(mean(x, na.rm=TRUE))==T, NA, mean(x, na.rm=TRUE))
Sum_value <- function(x) ifelse(sum(!is.na(x))==0, NA, sum(x, na.rm=TRUE))
Max_value <- function(x) ifelse(is.infinite(max(x, na.rm=TRUE))==T, NA, max(x, na.rm=TRUE))
Min_value <- function(x) ifelse(is.infinite(min(x, na.rm=TRUE))==T, NA, min(x, na.rm=TRUE))

# **************************************
# Constant
# **************************************
CURRENT_DATE = Sys.Date()
FUTURE_DATE = as.Date("3999-01-01")
MISSING = -9999999
EARLY_STOP_ROUNDS = 30
MEMB_NUM = 1000000000
```



## Data preparation 
```{r}
# **************************************
# Load data
# **************************************
setwd("~/Documents/github/kaggle-allstate/script/")

df_train = read_csv("../input/train.csv")
df_test = read_csv("../input/test.csv")
gc()

Y = df_train$loss
train_id = df_train$id
test_id = df_test$id

labels = df_train[,c("id", "loss")]
labels$label = labels$loss
labels$loss = NULL
df_train$loss = NULL




# **************************************
# List of numeric features and categorical features
# **************************************
ohe_feats = names(df_train %>% select(which(sapply(.,is.character))))
ohe_feats = setdiff(ohe_feats, "memb_num")

num_feats = setdiff(names(df_train), ohe_feats)
num_feats = setdiff(num_feats, "memb_num")


# **************************************
# Vectorize numeric features
# **************************************
df_all_num_feats = list()
i = 1
for(feat in num_feats){
  df_all_num_feats_ = df_train[c("memb_num", feat)]
  df_all_num_feats_$feature = feat
  df_all_num_feats_$value = as.numeric(df_all_num_feats_[[feat]])
  df_all_num_feats_ = df_all_num_feats_[c("memb_num", "feature", "value")]
  df_all_num_feats[[i]] = df_all_num_feats_
  i = i + 1
}
df_all_num_feats <- bind_rows(df_all_num_feats)
print("numeric feature")
print(n_distinct(df_all_num_feats$feature))

# **************************************
# Vectorize categorical features
# **************************************
df_all_ohe_feats = list()
i = 1
n_feats = 0
for(feat in ohe_feats){
  df_all_ohe_feats_ = df_train[c("memb_num", feat)]
  df_all_ohe_feats_$feature = paste(feat, df_all_ohe_feats_[[feat]], sep="_")
  n_feats_ = n_distinct(df_all_ohe_feats_$feature)
  df_all_ohe_feats_$value = 1
  df_all_ohe_feats_ = df_all_ohe_feats_[c("memb_num", "feature", "value")]
  df_all_ohe_feats[[i]] = df_all_ohe_feats_
  i = i + 1
  n_feats = n_feats + n_feats_
}
df_all_ohe_feats <- bind_rows(df_all_ohe_feats)
print("categorical feature")
print(n_feats)

# **************************************
# Sparse Matrix 
# **************************************
df_all_feats = 
  bind_rows(
    df_all_num_feats,
    df_all_ohe_feats
  )

X_all = df_all_feats
#X_all = subset(X_all, memb_num != "")
X_all = na.omit(X_all)
X_all$feature_name = X_all$feature
X_all$feature = as.numeric(as.factor(X_all$feature))
X_all$id_num = as.numeric(as.factor(X_all$memb_num))
X_all = X_all %>% arrange(id_num)

X_all_feature = X_all[!duplicated(X_all$feature), c("feature", "feature_name")]
X_all_feature = X_all_feature[order(X_all_feature$feature),]

X_all_id = X_all %>% distinct(id_num)
X_all_id = data.frame(memb_num = X_all_id$memb_num, id_num = X_all_id$id_num)
X_all_id = dplyr::left_join(X_all_id, labels, by = "memb_num")
X_all_id = X_all_id %>% arrange(id_num)

X_all_sp <- sparseMatrix(i = X_all$id_num,
                         j = X_all$feature,
                         x = X_all$value)
format(object.size(X_all_sp), units= "Mb")

#X_all_dense = data.frame(as.matrix(X_all_sp))
#format(object.size(X_all_dense), units= "Mb")



gc()

X_all_id_train = X_all_id %>% subset(X_all_id$memb_num > MEMB_NUM*10)
X_all_id_test = X_all_id %>% subset(X_all_id$memb_num < MEMB_NUM*10)


gc()
```







```{r}
dx_train = xgb.DMatrix(X_all_sp, 
                       label = X_all_id$label,
                       missing = MISSING)


## parameter setting
params = list(nthread = 8,
              booster = "gbtree",
              objective = "binary:logistic",
              max_depth = 3,
              eta = 0.3,
              colsample_bytree = 1,
              subsample = 0.7,
              seed = 6174)

## CV for small data set
model_cv = xgb.cv(params, dx_train, 50000, nfold=5,
      eval_metric = 'auc', maximize=T,
      early.stop.round = EARLY_STOP_ROUNDS, print.every.n = 1)


## Get the number of trees
model_cv_num = as.numeric(tail(row.names(model_cv), n=1)) - EARLY_STOP_ROUNDS

## Build tree
model = xgb.train(params = params, data = dx_train,
                  nrounds = model_cv_num,
                  eval_metric = 'auc', print.every.n = 100)

## Feature Importance
importance_matrix = xgb.importance(X_all_feature$feature_name, model = model)
importance_matrix

xgb.plot.importance(importance_matrix[1:50,])
#min(as.numeric(importance_matrix$Feature))
#importance_matrix = merge(data.frame(importance_matrix), X_all_feature, by.x = "Feature", by.y = "feature", all.x = T)
#importance_matrix = importance_matrix[order(importance_matrix$Gain, decreasing = T),]
#head(importance_matrix)

#737
```



```{r}
## Prediction
X_all_id_test = X_all_id %>% subset(label == 0)
X_all_id_test = X_all_id_test %>% arrange(id_num)
X_all_id_test_id_num = X_all_id_test$id_num

dx_test = xgb.DMatrix(X_all_sp[X_all_id_test_id_num, , drop = FALSE])


X_all_id_test$pred = predict(model, dx_test)
X_all_id_test[order(X_all_id_test$pred, decreasing = T), ][1:11,]

## Prediction 2 (Same as prediction, for double check)
X_all_id_backup = X_all_id
X_all_id_backup = X_all_id_backup %>% arrange(id_num)
X_all_id_backup$pred = predict(model, dx_train)
X_all_id_backup = X_all_id_backup %>% subset(label == 0)
X_all_id_backup[order(X_all_id_backup$pred, decreasing = T), ][1:11,]
X_all_id_backup$id_num = NULL
X_all_id_backup$label = NULL
head(X_all_id_backup)
```


```{r}
write_csv(X_all_id_backup, "~/Prediction/20161007_amex_similiarity_measure_final.csv")

```


```{r}
df_train = read_csv("~/Data/20160907_amex_train_m7.csv",
                    na = c("", " ", "NA","<NA>", "NULL", "null"),
                    col_types = list(curr_seniority = col_character()))

prediction = read_csv("~/Prediction/20160907_amex_similiarity_measure_6.csv",
                    na = c("", " ", "NA","<NA>", "NULL", "null"))

head(df_train)
head(prediction)

positive_cut_off = quantile(prediction$pred, 0.9)

prediction[prediction$pred >= positive_cut_off ,"pred"] = 1
prediction[prediction$pred < positive_cut_off ,"pred"] = 0

prediction 


```

