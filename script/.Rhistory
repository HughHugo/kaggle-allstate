library(htmlwidgets)
shiny::runApp('Documents/github/survey-text-analysis/nps')
library(JS)
library(DT)
?datatable
runApp('Documents/github/survey-text-analysis/nps')
?I
?parseFloat
?createLink
library(shiny)
library(DT)
?createLink
library(shinydashboard)
?css
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
shiny::runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
shiny::runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
shiny::runApp('Documents/github/survey-text-analysis/nps')
shiny::runApp('Documents/github/survey-text-analysis/nps')
library(DT)
runApp('Documents/github/survey-text-analysis/nps')
runApp('Documents/github/survey-text-analysis/nps')
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
rm(list = ls())  # Remove all objects
gc()             # Garbage Collection
library(xgboost)   # XGboost for Machine Learning
library(dplyr)     # Grammar of Data Manipulation
library(lubridate) # Date time handle
library(caret)      # caret
library(data.table) # Fast aggregation of large data
library(readr)     # Read Tabular Data
library(Matrix)    # Sparse Matrix
library(tidyr)
options(scipen=100)
options(dplyr.width = Inf)
#options(dplyr.print_max = Inf)
# **************************************
# cache folder
# **************************************
CACHE_DIR = "/Users/achm/Documents/github/bingo-models/R_campaign/amex/cache/"
# **************************************
# functions
# **************************************
'%nin%' <- Negate('%in%')
'%in_v%' <- function(x, y) x[x %in% y]
'%nin_v%' <- function(x, y) x[!x %in% y]
'%in_d%' <- function(x, y) x[names(x) %in% y]
'%nin_d%' <- function(x, y) x[!names(x) %in% y]
'%+%' <- function(x, y) paste0(x, y)
Mean <- function(x) mean(x, na.rm=TRUE)
Median <- function(x) median(x, na.rm=TRUE)
Sd <- function(x) sd(x, na.rm=TRUE)
Sum <- function(x) sum(x, na.rm=TRUE)
Max <- function(x) max(x, na.rm=TRUE)
Min <- function(x) min(x, na.rm=TRUE)
Mean_value <- function(x) ifelse(is.nan(mean(x, na.rm=TRUE))==T, NA, mean(x, na.rm=TRUE))
Sum_value <- function(x) ifelse(sum(!is.na(x))==0, NA, sum(x, na.rm=TRUE))
Max_value <- function(x) ifelse(is.infinite(max(x, na.rm=TRUE))==T, NA, max(x, na.rm=TRUE))
Min_value <- function(x) ifelse(is.infinite(min(x, na.rm=TRUE))==T, NA, min(x, na.rm=TRUE))
# **************************************
# Constant
# **************************************
CURRENT_DATE = Sys.Date()
FUTURE_DATE = as.Date("3999-01-01")
MISSING = -9999999
EARLY_STOP_ROUNDS = 30
MEMB_NUM = 1000000000
# Chunk 3
df_train = read_csv("~/Data/20161007_amex_train_m7.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train$first_am_join_date = as.numeric(df_train$start_date - df_train$first_am_join_date)
tmp = df_train[df_train$amex_holder == 1, ]
summary(tmp$first_am_join_date)
sum(tmp$first_am_join_date ==0)
tmp_2 = df_train[df_train$amex_holder == 0, ]
summary(tmp_2$first_am_join_date)
sum(tmp_2$first_am_join_date ==0)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
rm(list = ls())  # Remove all objects
gc()             # Garbage Collection
library(xgboost)   # XGboost for Machine Learning
library(dplyr)     # Grammar of Data Manipulation
library(lubridate) # Date time handle
library(caret)      # caret
library(data.table) # Fast aggregation of large data
library(readr)     # Read Tabular Data
library(Matrix)    # Sparse Matrix
library(tidyr)
options(scipen=100)
options(dplyr.width = Inf)
#options(dplyr.print_max = Inf)
# **************************************
# cache folder
# **************************************
CACHE_DIR = "/Users/achm/Documents/github/bingo-models/R_campaign/amex/cache/"
# **************************************
# functions
# **************************************
'%nin%' <- Negate('%in%')
'%in_v%' <- function(x, y) x[x %in% y]
'%nin_v%' <- function(x, y) x[!x %in% y]
'%in_d%' <- function(x, y) x[names(x) %in% y]
'%nin_d%' <- function(x, y) x[!names(x) %in% y]
'%+%' <- function(x, y) paste0(x, y)
Mean <- function(x) mean(x, na.rm=TRUE)
Median <- function(x) median(x, na.rm=TRUE)
Sd <- function(x) sd(x, na.rm=TRUE)
Sum <- function(x) sum(x, na.rm=TRUE)
Max <- function(x) max(x, na.rm=TRUE)
Min <- function(x) min(x, na.rm=TRUE)
Mean_value <- function(x) ifelse(is.nan(mean(x, na.rm=TRUE))==T, NA, mean(x, na.rm=TRUE))
Sum_value <- function(x) ifelse(sum(!is.na(x))==0, NA, sum(x, na.rm=TRUE))
Max_value <- function(x) ifelse(is.infinite(max(x, na.rm=TRUE))==T, NA, max(x, na.rm=TRUE))
Min_value <- function(x) ifelse(is.infinite(min(x, na.rm=TRUE))==T, NA, min(x, na.rm=TRUE))
# **************************************
# Constant
# **************************************
CURRENT_DATE = Sys.Date()
FUTURE_DATE = as.Date("3999-01-01")
MISSING = -9999999
EARLY_STOP_ROUNDS = 30
MEMB_NUM = 1000000000
# Chunk 3
df_train = read_csv("~/Data/20161007_amex_train_m7.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train$first_am_join_date = as.numeric(df_train$start_date - df_train$first_am_join_date)
tmp = df_train[df_train$amex_holder == 1, ]
summary(tmp$first_am_join_date)
sum(tmp$first_am_join_date ==0)
tmp_2 = df_train[df_train$amex_holder == 0, ]
summary(tmp_2$first_am_join_date)
sum(tmp_2$first_am_join_date ==0)
# Chunk 4
# **************************************
# Load data
# **************************************
df_train_1 = read_csv("~/Data/20161007_amex_train_m7.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train_2 = read_csv("~/Data/20161007_amex_train_m13.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train_2 = df_train_2[,c("memb_num", names(df_train_2)[!names(df_train_2) %in% names(df_train_1)])]
df_train = df_train_1 %>% left_join(df_train_2, by="memb_num")
df_train_1 = NULL
df_train_2 = NULL
gc()
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
rm(list = ls())  # Remove all objects
gc()             # Garbage Collection
library(xgboost)   # XGboost for Machine Learning
library(dplyr)     # Grammar of Data Manipulation
library(lubridate) # Date time handle
library(caret)      # caret
library(data.table) # Fast aggregation of large data
library(readr)     # Read Tabular Data
library(Matrix)    # Sparse Matrix
library(tidyr)
options(scipen=100)
options(dplyr.width = Inf)
#options(dplyr.print_max = Inf)
# **************************************
# cache folder
# **************************************
CACHE_DIR = "/Users/achm/Documents/github/bingo-models/R_campaign/amex/cache/"
# **************************************
# functions
# **************************************
'%nin%' <- Negate('%in%')
'%in_v%' <- function(x, y) x[x %in% y]
'%nin_v%' <- function(x, y) x[!x %in% y]
'%in_d%' <- function(x, y) x[names(x) %in% y]
'%nin_d%' <- function(x, y) x[!names(x) %in% y]
'%+%' <- function(x, y) paste0(x, y)
Mean <- function(x) mean(x, na.rm=TRUE)
Median <- function(x) median(x, na.rm=TRUE)
Sd <- function(x) sd(x, na.rm=TRUE)
Sum <- function(x) sum(x, na.rm=TRUE)
Max <- function(x) max(x, na.rm=TRUE)
Min <- function(x) min(x, na.rm=TRUE)
Mean_value <- function(x) ifelse(is.nan(mean(x, na.rm=TRUE))==T, NA, mean(x, na.rm=TRUE))
Sum_value <- function(x) ifelse(sum(!is.na(x))==0, NA, sum(x, na.rm=TRUE))
Max_value <- function(x) ifelse(is.infinite(max(x, na.rm=TRUE))==T, NA, max(x, na.rm=TRUE))
Min_value <- function(x) ifelse(is.infinite(min(x, na.rm=TRUE))==T, NA, min(x, na.rm=TRUE))
# **************************************
# Constant
# **************************************
CURRENT_DATE = Sys.Date()
FUTURE_DATE = as.Date("3999-01-01")
MISSING = -9999999
EARLY_STOP_ROUNDS = 30
MEMB_NUM = 1000000000
# Chunk 3
df_train = read_csv("~/Data/20161007_amex_train_m7.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train$first_am_join_date = as.numeric(df_train$start_date - df_train$first_am_join_date)
tmp = df_train[df_train$amex_holder == 1, ]
summary(tmp$first_am_join_date)
sum(tmp$first_am_join_date ==0)
tmp_2 = df_train[df_train$amex_holder == 0, ]
summary(tmp_2$first_am_join_date)
sum(tmp_2$first_am_join_date ==0)
# Chunk 4
# **************************************
# Load data
# **************************************
df_train_1 = read_csv("~/Data/20161007_amex_train_m7.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train_2 = read_csv("~/Data/20161007_amex_train_m13.csv",
na = c("", " ", "NA","<NA>", "NULL", "null"),
col_types = list(curr_seniority = col_character()))
df_train_2 = df_train_2[,c("memb_num", names(df_train_2)[!names(df_train_2) %in% names(df_train_1)])]
df_train = df_train_1 %>% left_join(df_train_2, by="memb_num")
df_train_1 = NULL
df_train_2 = NULL
gc()
# Distinguish train test
df_train[df_train$amex_holder==1, "memb_num"] = as.numeric(paste0(df_train[df_train$amex_holder==1,]$memb_num,
as.numeric(df_train[df_train$amex_holder==1,]$start_date)))
df_train = df_train %>% subset(!duplicated(df_train$memb_num))
length(unique(df_train$memb_num))
dim(df_train)
# prefred mail country == HK
df_train = df_train[df_train$prefred_mail_ctry == "HK",]
table(df_train[df_train$amex_holder == 0,]$prefred_mail_ctry)
table(df_train[df_train$amex_holder == 1,]$prefred_mail_ctry)
df_train$prefred_mail_ctry = NULL
labels = df_train[,c("memb_num", "amex_holder")]
df_train$amex_holder = NULL
labels$label = labels$amex_holder
labels$amex_holder = NULL
#leakage
df_train$enrol_src = NULL
df_train$email_mesg_pref = NULL
df_train$telemkt_pref = NULL
df_train$promo_mail_ind = NULL
df_train$prefred_mail_addr_ind = NULL
df_train$prefered_email_type = NULL
df_train$prev_prefred_mail_ctry = NULL
# **************************************
# Date Transformation
# **************************************
df_train$start_date = as.Date(df_train$start_date)
# first_join_date
#df_train$first_join_date = as.Date(df_train$first_join_date)
#df_train$first_join_date = as.numeric(df_train$start_date  - df_train$first_join_date)
#df_train[!is.na(df_train$first_join_date) & (df_train$first_join_date)<0, "first_join_date"] = 0
df_train$first_join_date = NULL
# first_club_join_date
#df_train$first_club_join_date = as.Date(df_train$first_club_join_date)
#df_train$first_club_join_date = as.numeric(df_train$start_date - df_train$first_club_join_date)
#df_train[!is.na(df_train$first_club_join_date) & (df_train$first_club_join_date)<0, "first_club_join_date"] = NA
df_train$first_club_join_date = NULL
# first_am_join_date
df_train$first_am_join_date = as.Date(df_train$first_am_join_date)
df_train$first_am_join_date = as.numeric(df_train$start_date - df_train$first_am_join_date)
df_train[df_train$first_am_join_date == 0, "first_am_join_date"] = 1
# birth yr
df_train$birth_yr = year(df_train$start_date) - df_train$birth_yr
df_train$start_date = NULL
#df_train$first_join_date = NULL
#df_train$first_join_date_year = NULL
#df_train$first_club_join_date = NULL
#df_train$first_club_join_date_year = NULL
#df_train$first_am_join_date = NULL
#df_train$first_am_join_date_year = NULL
# **************************************
# List of numeric features and categorical features
# **************************************
ohe_feats = names(df_train %>% select(which(sapply(.,is.character))))
ohe_feats = setdiff(ohe_feats, "memb_num")
num_feats = setdiff(names(df_train), ohe_feats)
num_feats = setdiff(num_feats, "memb_num")
# **************************************
# Vectorize numeric features
# **************************************
df_all_num_feats = list()
i = 1
for(feat in num_feats){
df_all_num_feats_ = df_train[c("memb_num", feat)]
df_all_num_feats_$feature = feat
df_all_num_feats_$value = as.numeric(df_all_num_feats_[[feat]])
df_all_num_feats_ = df_all_num_feats_[c("memb_num", "feature", "value")]
df_all_num_feats[[i]] = df_all_num_feats_
i = i + 1
}
df_all_num_feats <- bind_rows(df_all_num_feats)
print("numeric feature")
print(n_distinct(df_all_num_feats$feature))
# **************************************
# Vectorize categorical features
# **************************************
df_all_ohe_feats = list()
i = 1
n_feats = 0
for(feat in ohe_feats){
df_all_ohe_feats_ = df_train[c("memb_num", feat)]
df_all_ohe_feats_$feature = paste(feat, df_all_ohe_feats_[[feat]], sep="_")
n_feats_ = n_distinct(df_all_ohe_feats_$feature)
df_all_ohe_feats_$value = 1
df_all_ohe_feats_ = df_all_ohe_feats_[c("memb_num", "feature", "value")]
df_all_ohe_feats[[i]] = df_all_ohe_feats_
i = i + 1
n_feats = n_feats + n_feats_
}
df_all_ohe_feats <- bind_rows(df_all_ohe_feats)
print("categorical feature")
print(n_feats)
# **************************************
# Sparse Matrix
# **************************************
df_all_feats =
bind_rows(
df_all_num_feats,
df_all_ohe_feats
)
X_all = df_all_feats
#X_all = subset(X_all, memb_num != "")
X_all = na.omit(X_all)
X_all$feature_name = X_all$feature
X_all$feature = as.numeric(as.factor(X_all$feature))
X_all$id_num = as.numeric(as.factor(X_all$memb_num))
X_all = X_all %>% arrange(id_num)
X_all_feature = X_all[!duplicated(X_all$feature), c("feature", "feature_name")]
X_all_feature = X_all_feature[order(X_all_feature$feature),]
X_all_id = X_all %>% distinct(id_num)
X_all_id = data.frame(memb_num = X_all_id$memb_num, id_num = X_all_id$id_num)
X_all_id = dplyr::left_join(X_all_id, labels, by = "memb_num")
X_all_id = X_all_id %>% arrange(id_num)
X_all_sp <- sparseMatrix(i = X_all$id_num,
j = X_all$feature,
x = X_all$value)
format(object.size(X_all_sp), units= "Mb")
#X_all_dense = data.frame(as.matrix(X_all_sp))
#format(object.size(X_all_dense), units= "Mb")
gc()
X_all_id_train = X_all_id %>% subset(X_all_id$memb_num > MEMB_NUM*10)
X_all_id_test = X_all_id %>% subset(X_all_id$memb_num < MEMB_NUM*10)
gc()
# Chunk 5
dx_train = xgb.DMatrix(X_all_sp,
label = X_all_id$label,
missing = MISSING)
## parameter setting
params = list(nthread = 8,
booster = "gbtree",
objective = "binary:logistic",
max_depth = 3,
eta = 0.3,
colsample_bytree = 1,
subsample = 0.7,
seed = 6174)
## CV for small data set
model_cv = xgb.cv(params, dx_train, 50000, nfold=5,
eval_metric = 'auc', maximize=T,
early.stop.round = EARLY_STOP_ROUNDS, print.every.n = 1)
## Get the number of trees
model_cv_num = as.numeric(tail(row.names(model_cv), n=1)) - EARLY_STOP_ROUNDS
## Build tree
model = xgb.train(params = params, data = dx_train,
nrounds = model_cv_num,
eval_metric = 'auc', print.every.n = 100)
## Feature Importance
importance_matrix = xgb.importance(X_all_feature$feature_name, model = model)
importance_matrix
xgb.plot.importance(importance_matrix[1:50,])
#min(as.numeric(importance_matrix$Feature))
#importance_matrix = merge(data.frame(importance_matrix), X_all_feature, by.x = "Feature", by.y = "feature", all.x = T)
#importance_matrix = importance_matrix[order(importance_matrix$Gain, decreasing = T),]
#head(importance_matrix)
#737
# Chunk 6
## Prediction
X_all_id_test = X_all_id %>% subset(label == 0)
X_all_id_test = X_all_id_test %>% arrange(id_num)
X_all_id_test_id_num = X_all_id_test$id_num
dx_test = xgb.DMatrix(X_all_sp[X_all_id_test_id_num, , drop = FALSE])
X_all_id_test$pred = predict(model, dx_test)
X_all_id_test[order(X_all_id_test$pred, decreasing = T), ][1:11,]
## Prediction 2 (Same as prediction, for double check)
X_all_id_backup = X_all_id
X_all_id_backup = X_all_id_backup %>% arrange(id_num)
X_all_id_backup$pred = predict(model, dx_train)
X_all_id_backup = X_all_id_backup %>% subset(label == 0)
X_all_id_backup[order(X_all_id_backup$pred, decreasing = T), ][1:11,]
X_all_id_backup$id_num = NULL
X_all_id_backup$label = NULL
head(X_all_id_backup)
# Chunk 7
write_csv(X_all_id_backup, "~/Prediction/20161007_amex_similiarity_measure_final.csv")
dim(X_all_id_backup)
head(X_all_id_backup)
c(15,12,8,8,7,7,7,6,5,3)
a=c(15,12,8,8,7,7,7,6,5,3)
b=c(10,25,17,11,13,166,20,13,9,15)
cor(a,b)
b=c(10,25,17,11,13,17,20,13,9,15)
cor(a,b)
sd(a)*sd(b)
sd(a)*sd(b)/9
sd(a)*sd(b)*9
sd(a)
sum(a)- mean(a)
1/9* (sum(a)- mean(a))**2
sqrt(1/9* (sum(a)- mean(a))**2)
sd(a)
sqrt(1/9* (sum(a)- mean(a))**2)
sqrt(1/9* (sum(a- mean(a))**2)0
sqrt(1/9* (sum(a- mean(a))**2))
1/9* (sum(a- mean(a))**2)
(sum(a- mean(a))**2)
a
mean(a)
(sum(a- mean(a)**2))
(sum(a - mean(a)^2))
(sum((a - mean(a))^2))
sqrt(1/9*(sum((a - mean(a))^2)))
sd(a)
cor(a,b)
shiny::runApp('Documents/github/survey-text-analysis/nps')
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
# Chunk 2
rm(list = ls())  # Remove all objects
gc()             # Garbage Collection
library(xgboost)   # XGboost for Machine Learning
library(dplyr)     # Grammar of Data Manipulation
library(lubridate) # Date time handle
library(caret)      # caret
library(data.table) # Fast aggregation of large data
library(readr)     # Read Tabular Data
library(Matrix)    # Sparse Matrix
library(tidyr)
options(scipen=100)
options(dplyr.width = Inf)
#options(dplyr.print_max = Inf)
# **************************************
# functions
# **************************************
'%nin%' <- Negate('%in%')
'%in_v%' <- function(x, y) x[x %in% y]
'%nin_v%' <- function(x, y) x[!x %in% y]
'%in_d%' <- function(x, y) x[names(x) %in% y]
'%nin_d%' <- function(x, y) x[!names(x) %in% y]
'%+%' <- function(x, y) paste0(x, y)
Mean <- function(x) mean(x, na.rm=TRUE)
Median <- function(x) median(x, na.rm=TRUE)
Sd <- function(x) sd(x, na.rm=TRUE)
Sum <- function(x) sum(x, na.rm=TRUE)
Max <- function(x) max(x, na.rm=TRUE)
Min <- function(x) min(x, na.rm=TRUE)
Mean_value <- function(x) ifelse(is.nan(mean(x, na.rm=TRUE))==T, NA, mean(x, na.rm=TRUE))
Sum_value <- function(x) ifelse(sum(!is.na(x))==0, NA, sum(x, na.rm=TRUE))
Max_value <- function(x) ifelse(is.infinite(max(x, na.rm=TRUE))==T, NA, max(x, na.rm=TRUE))
Min_value <- function(x) ifelse(is.infinite(min(x, na.rm=TRUE))==T, NA, min(x, na.rm=TRUE))
# **************************************
# Constant
# **************************************
CURRENT_DATE = Sys.Date()
FUTURE_DATE = as.Date("3999-01-01")
MISSING = -9999999
EARLY_STOP_ROUNDS = 30
MEMB_NUM = 1000000000
setwd("~/Documents/github/kaggle-allstate/script/")
df_train_1 = read_csv("../input/train.csv")
df_train = read_csv("../input/train.csv")
gc()
head(df_train)
setwd("~/Documents/github/kaggle-allstate/script/")
df_train = read_csv("../input/train.csv")
df_test = read_csv("../input/test.csv")
gc()
setwd("~/Documents/github/kaggle-allstate/script/")
df_train = read_csv("../input/train.csv")
df_test = read_csv("../input/test.csv")
gc()
Y = df_train$loss
train_id = df_train$id
test_id = df_test$id
labels = df_train[,c("id", "loss")]
labels$label = labels$loss
labels$loss = NULL
df_train$loss = NULL
df_train
head(df_train)
head(Df_train)
head(df_train)
df_train[,c('id','loss')]
df_train[,c('id','loss')]
df_train = read_csv("../input/train.csv")
df_train[,c('id','loss')]
